{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snowflake.snowpark\n",
    "from snowflake.snowpark.session import Session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark.functions import udf, udtf\n",
    "from snowflake.snowpark.types import IntegerType, StringType, VariantType, DateType, PandasSeries, PandasSeriesType, StructField, StructType\n",
    "from snowflake.snowpark.functions import table_function\n",
    "\n",
    "from datetime import date\n",
    "from tokenize import String\n",
    "import json\n",
    "import pandas\n",
    "import zipfile\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import cachetools\n",
    "from joblib import load\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_parameters = json.load(open('creds.json'))\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the stage for python and model data\n",
    "session.sql('create stage if not exists raw_data').collect()\n",
    "session.sql('create stage if not exists model_data').collect()\n",
    "session.sql('create stage if not exists python_load').collect()\n",
    "\n",
    "# create the directory stage for the data\n",
    "session.sql('create stage if not exists raw_data_stage directory = (enable = true)').collect()\n",
    "\n",
    "# upload the unstructured file and stop words to the stages\n",
    "session.file.put('reviews__0_0_0.dat','@raw_data_stage',auto_compress=False)\n",
    "session.file.put('en_core_web_sm.zip','@model_data')\n",
    "\n",
    "# refresh the stage\n",
    "session.sql('alter stage raw_data_stage refresh').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep\n",
    "\n",
    "We'll start this demo by first building our sentiment model, in order to do this we have a set of training data containing previous reviews and their classification for sentiment that requires cleaning and transforming.\n",
    "\n",
    "First we'll need to refine the text (remove punctuation, stopwords etc.) and then we'll want to make the sentiment classification more suitable for our algoritm. \n",
    "\n",
    "---\n",
    "To get started, lets take a look at the training data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table(\"TRAINING_DATA\").show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check the distribution of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df = session.table('TRAINING_DATA') \\\n",
    "    .group_by(F.col('SENTIMENT')) \\\n",
    "    .agg(F.count(F.col('PRODUCT_ID')).alias('COUNT')).to_pandas()\n",
    "\n",
    "sns.set(rc={'figure.figsize':(20,10)})\n",
    "sns.barplot(x='SENTIMENT',y='COUNT',data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see we have various reviews for products with their corresponding sentiment classification.\n",
    "\n",
    "---\n",
    "\n",
    "The first transformation will be to process the review text. To do this we create a UDF that will perform the following:\n",
    "\n",
    "- Remove stop words\n",
    "- Remove punctuation\n",
    "- Remove currency values\n",
    "- Lemmatize the text\n",
    "\n",
    "<br>\n",
    "\n",
    "Note that we create a vectorized UDF, so we can take advantage of batch processing in the UDF, additionally we cache the stopwords lexicon for better performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "session.add_import('@model_data/en_core_web_sm.zip.gz')\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_file(import_dir):\n",
    "    input_file = import_dir + 'en_core_web_sm.zip'\n",
    "    output_dir = '/tmp/en_core_web_sm' + str(os.getpid())\n",
    "            \n",
    "    with zipfile.ZipFile(input_file, 'r') as zip_ref:\n",
    "        zip_ref.extractall(output_dir)\n",
    "        \n",
    "    return spacy.load(output_dir + \"/en_core_web_sm/en_core_web_sm-2.3.0\")    \n",
    "\n",
    "@udf(name='remove_stopwords_vect',packages=['spacy==2.3.5','cachetools'], session=session, is_permanent=True, replace=True, max_batch_size=1000,stage_location='python_load',)\n",
    "def remove_stopwords_vect(raw_text: PandasSeries[str]) -> PandasSeries[str]:\n",
    "    nlp = load_file(sys._xoptions['snowflake_import_directory'])\n",
    "    stop_words = nlp.Defaults.stop_words\n",
    "\n",
    "    result = []\n",
    "    \n",
    "    for s in raw_text:\n",
    "        doc = nlp(s)\n",
    "        text = [str(t.lemma_) for t in doc if  \n",
    "                t not in stop_words \n",
    "                and not t.is_punct \n",
    "                and not t.is_currency\n",
    "                and t.lemma_ != '-PRON-']\n",
    "        text = list(map(lambda x: x.replace(' ', ''), text))\n",
    "        text = list(map(lambda x: x.replace('\\n', ''), text))\n",
    "        result.append(' '.join(token.lower() for token in text))\n",
    "    \n",
    "    return pandas.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can use this on text to see how this is processed: `This surfboard is amazing. I can ride some epic waves on this thing` will turn into:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.sql('''select remove_stopwords_vect('This surfboard is awesome. I can ride some epic waves on this thing and I only paid $1,000') as processed_text''').toPandas()\n",
    "str_sentiment = df.iat[0,0]\n",
    "print(str_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "The next transformation we'll need to do is convert the string value for sentiment into a numeric value, in order to make it more optimized for our ML algorithm. \n",
    "\n",
    "To do this we can create a simple UDF to bin the sentiment string to a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload the UDF to bin the rating to sentiment \n",
    "@udf(name='convert_rating',\n",
    "     is_permanent=True,\n",
    "     replace=True,\n",
    "     stage_location='python_load')\n",
    "\n",
    "def convert_rating(x: str) -> int:\n",
    "    if x == 'NEGATIVE':\n",
    "        return -1\n",
    "    elif x == 'NEUTRAL':\n",
    "        return 0\n",
    "    elif x == 'POSITIVE':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "With these UDFs we can now run a query and see what our data will look like for training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = session.table('TRAINING_DATA') \\\n",
    "    .filter(\n",
    "        F.col('REVIEWTEXT') != ''\n",
    "    ) \\\n",
    "    .select( \\\n",
    "        F.col('PRODUCT_ID'),\n",
    "        F.col('REVIEWDATE'),\n",
    "        F.call_udf(\n",
    "            'REMOVE_STOPWORDS_VECT',\n",
    "            F.col('REVIEWTEXT')).alias('PROCESSED_REVIEW'),\n",
    "        F.call_udf(\n",
    "            'CONVERT_RATING',\n",
    "            F.col('SENTIMENT')).alias('SENTIMENT')).show(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Training\n",
    "\n",
    "Next we want to train a model. Doing this in Snowflake is as simple are creating a Python Stored Procedure, which also allows us to re-run this when we want to retrain the model. Model training uses Snowflake Compute.\n",
    "\n",
    "The model will be saved to an internal stage, and can be used in a UDF for model inference within Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and upload a stored proc to train our sentiment model\n",
    "import sklearn\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import os\n",
    "from joblib import dump\n",
    "\n",
    "def save_file(session, model, path):\n",
    "    input_stream = io.BytesIO()\n",
    "    pickle.dump(model, input_stream)\n",
    "    session._conn._cursor.upload_stream(input_stream, path)\n",
    "    \n",
    "def train_sentiment_model(session: snowflake.snowpark.Session) -> float:        \n",
    "    # build a pd with review data\n",
    "    df = session.table('TRAINING_DATA') \\\n",
    "        .filter(\n",
    "            F.col('REVIEWTEXT') != '') \\\n",
    "        .select(\n",
    "            F.call_udf(\n",
    "                'REMOVE_STOPWORDS_VECT',\n",
    "                F.col('REVIEWTEXT')).alias('PROCESSED_TEXT'),\n",
    "            F.call_udf(\n",
    "                'CONVERT_RATING',\n",
    "                F.col('SENTIMENT')).alias('SENTIMENT')).toPandas()\n",
    "    \n",
    "    index = df.index\n",
    "    df['RANDOM'] = np.random.randn(len(index))\n",
    "    train = df[df['RANDOM'] <= 0.8] # 0.8\n",
    "    test = df[df['RANDOM'] > 0.8] # 0.8\n",
    "    \n",
    "    # vectorize the data\n",
    "    vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "    train_matrix = vectorizer.fit_transform(train['PROCESSED_TEXT'])\n",
    "    test_matrix = vectorizer.transform(test['PROCESSED_TEXT'])\n",
    "    \n",
    "    # split feature and label \n",
    "    x_train = train_matrix\n",
    "    x_test = test_matrix\n",
    "    y_train = train['SENTIMENT']\n",
    "    y_test = test['SENTIMENT']\n",
    "    \n",
    "    # Logistic Regression Model\n",
    "    lr = LogisticRegression(multi_class='multinomial', max_iter=10000)\n",
    "    lr.fit(x_train,y_train)\n",
    "    predictions = lr.predict(x_test)\n",
    "\n",
    "    model_output_dir = '/tmp'\n",
    "\n",
    "    # Save model file\n",
    "    model_file = os.path.join(model_output_dir, 'model.joblib')\n",
    "    dump(lr, model_file)\n",
    "    session.file.put(model_file, \"@model_data\",overwrite=True)\n",
    "\n",
    "    # Save vectorizer file\n",
    "    vect_file = os.path.join(model_output_dir, 'vectorizer.joblib')\n",
    "    dump(vectorizer, vect_file)\n",
    "    session.file.put(vect_file, \"@model_data\",overwrite=True)\n",
    "\n",
    "    return accuracy_score(y_test, predictions)\n",
    "\n",
    "# Register the Stored Procedure\n",
    "session.sproc.register(name='train_sentiment_model',\n",
    "                       func=train_sentiment_model, \n",
    "                       packages=['snowflake-snowpark-python','pandas','scikit-learn', 'joblib'],\n",
    "                       replace=True, \n",
    "                       is_permanent=True,\n",
    "                       stage_location='python_load')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "session.call('TRAIN_SENTIMENT_MODEL')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Model Deployment\n",
    "\n",
    "With the model artifact produced from the Stored Procedure, we can create a UDF that can be used to infer sentiment for future data ingested into Snowflake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.clear_packages()\n",
    "session.clear_imports()\n",
    "session.add_import('@MODEL_DATA/model.joblib.gz')\n",
    "session.add_import('@MODEL_DATA/vectorizer.joblib.gz')\n",
    "\n",
    "@cachetools.cached(cache={})\n",
    "def load_model(file_name):\n",
    "    model_file_path = sys._xoptions.get(\"snowflake_import_directory\") + file_name\n",
    "    return load(model_file_path)\n",
    "\n",
    "columns = ('NEGATIVE','NEUTRAL','POSITIVE')\n",
    "    \n",
    "@udf(name='predict_sentiment_vect',\n",
    "     is_permanent=True,\n",
    "     replace=True,\n",
    "     stage_location='python_load',\n",
    "     max_batch_size=50000,\n",
    "     input_types=[PandasSeriesType(StringType())], \n",
    "     return_type=PandasSeriesType(VariantType()),\n",
    "     packages=['pandas','scikit-learn','cachetools','joblib'])     \n",
    "def predict_sentiment_vector(sentiment_str):  \n",
    "    model = load_model('model.joblib.gz')\n",
    "    vectorizer = load_model('vectorizer.joblib.gz')                            \n",
    "    \n",
    "    result = []\n",
    "    \n",
    "    for s in sentiment_str:        \n",
    "        matrix = vectorizer.transform([s])\n",
    "        \n",
    "        df = pd.DataFrame(model.predict_proba(matrix),columns=columns)\n",
    "                \n",
    "        response = df.loc[0].to_json()\n",
    "        result.append(json.loads(response))\n",
    "        \n",
    "    return pandas.Series(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly test our UDF with a simple SQL call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "|\"SENTIMENT\"                 |\n",
      "------------------------------\n",
      "|{                           |\n",
      "|  \"NEGATIVE\": 0.08606723,   |\n",
      "|  \"NEUTRAL\": 0.1035442952,  |\n",
      "|  \"POSITIVE\": 0.8103884748  |\n",
      "|}                           |\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''select predict_sentiment_vect('PRACTICALLY PERFECT IN EVERY WAY') sentiment''').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingesting new data for scoring\n",
    "\n",
    "Now the model is trained and deployed, we can begin ingesting our unstructured data and using our new arctifacts - all within Snowflake, using Snowflake compute infrastructure. \n",
    "\n",
    "---\n",
    "\n",
    "First we create a UDTF that using the `snowflake` class which exposes functions allowing us to read data from unstructued files. We're then able to pull our data elements and return this as part of the UDTF response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the UDTF to read the file\n",
    "schema = StructType([\n",
    "    StructField(\"product_id\", StringType()),\n",
    "    StructField(\"review_date\", DateType()),\n",
    "    StructField(\"product_review\", StringType())   \n",
    "])\n",
    "\n",
    "@udtf(name = \"read_unstructured_reviews\",is_permanent = True, session=session, stage_location=\"model_data\", replace=True, input_types=[StringType()], output_schema=schema)\n",
    "class read_reviews:\n",
    "    def process(self, stagefile):\n",
    "        import _snowflake\n",
    "        \n",
    "        with _snowflake.open(stagefile) as f:\n",
    "            data = f.readall().decode('utf-8')\n",
    "            lines = data.split('\\n')\n",
    "            for line in lines:\n",
    "                lineStr = line.strip()\n",
    "                d = lineStr.split(\"|\")\n",
    "                try:\n",
    "                    # Read the product_id, the product review and the review date.                    \n",
    "                    review_date = date.fromisoformat(d[1])\n",
    "                    product_id = d[0]                    \n",
    "                    product_review = d[2]\n",
    "                    yield (product_id, review_date, product_review, )\n",
    "                except:\n",
    "                    pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a view that queries the Directory Table (this is the only way at present to query Directory Tables in Snowpark) we can get Stage URLs that can be passed to our UDTF. The Directory table looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------\n",
      "|\"RELATIVE_PATH\"     |\"FILE_URL\"                                          |\n",
      "---------------------------------------------------------------------------\n",
      "|reviews__0_0_0.dat  |https://pza13411.us-east-1.privatelink.snowflak...  |\n",
      "---------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sql('''\n",
    "    select \n",
    "        relative_path,\n",
    "        file_url\n",
    "    from\n",
    "        directory(@raw_data_stage)\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------------\n",
      "|\"RELATIVE_PATH\"     |\"PRODUCT_ID\"  |\"REVIEW_DATE\"  |\"PRODUCT_REVIEW\"                                    |\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-02     |I've used Tenergy in the past for specialty Li-...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-04     |Despite reports and complaints regarding the in...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-04     |I got these for a fenix flashlight, and so far ...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-04     |I have had really good luck with these , which ...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-08     |These batteries are great. The price is right a...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-10     |These batteries last a good long time in my tac...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-11     |Much cheaper than battery plus, I wear these ou...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-12     |It's not Energizer its not Duracell but for the...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-12     |All batteries arrived quickly and so far all op...  |\n",
      "|reviews__0_0_0.dat  |B001W9Y4PK    |2013-04-13     |I don't know how the charge will hold up yet, b...  |\n",
      "----------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.sql('''\n",
    "    select \n",
    "        relative_path,\n",
    "        product_id,\n",
    "        review_date,        \n",
    "        product_review\n",
    "    from \n",
    "        directory(@raw_data_stage) f,\n",
    "        table(read_unstructured_reviews(f.file_url))\n",
    "    order by 1,2,3\n",
    "''')\n",
    "\n",
    "df_temp = df\n",
    "df_temp.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.save_as_table('new_reviews_raw',mode=\"overwrite\", table_type=\"temporary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.table('new_reviews_raw').select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'), \n",
    "    F.call_udf(\n",
    "        'REMOVE_STOPWORDS_VECT',\n",
    "        F.col('PRODUCT_REVIEW')).alias('PROCESSED_REVIEW')    \n",
    ").write.save_as_table('new_reviews_ready',mode=\"overwrite\", table_type=\"temporary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"PRODUCT_ID\"  |\"REVIEW_DATE\"  |\"PRODUCT_REVIEW\"                                    |\"POSITIVE\"             |\"NEUTRAL\"              |\"NEGATIVE\"             |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|B0023B14TU    |2010-01-02     |We have an HD Flip Video which we bought at the...  |0.1427829016           |0.2248342077           |0.6323828907           |\n",
      "|B0023B14TU    |2010-01-02     |The camera is perfect for portable recording.  ...  |0.8444131441           |0.1553306645           |0.0002561914           |\n",
      "|B0023B14TU    |2010-01-02     |I purchased this flip camcorder because it had ...  |0.9990063715           |0.0009583783           |3.525010000000000e-05  |\n",
      "|B0023B14TU    |2010-01-03     |I bought myself a Flip camcorder, because it wa...  |0.9483641793           |0.0411627574           |0.0104730633           |\n",
      "|B0023B14TU    |2010-01-03     |This little camcorder is wonderful!  I had it o...  |0.9999854093           |1.446900000000000e-05  |1.217000000000000e-07  |\n",
      "|B0023B14TU    |2010-01-03     |After weeks of research and reading the reviews...  |0.3667401167           |0.4067539785           |0.2265059049           |\n",
      "|B0023B14TU    |2010-01-04     |Amazingly convenient.  Great picture for its si...  |0.8995930971           |0.0887559373           |0.0116509656           |\n",
      "|B0023B14TU    |2010-01-04     |Received it as a Christmas gift.  Plugged it in...  |0.0001537199           |0.0023108387           |0.9975354415           |\n",
      "|B0023B14TU    |2010-01-05     |I bought this camcorder for my daughter for Chr...  |0.9918796034           |0.0045165935           |0.0036038031           |\n",
      "|B0023B14TU    |2010-01-05     |We recently purchased the Flip UltraHD to use p...  |0.5652786252           |0.3986263832           |0.0360949915           |\n",
      "|B0023B14TU    |2010-01-05     |Bought this for my wife who wanted a quick simp...  |0.8772495857           |0.0322101457           |0.0905402686           |\n",
      "|B0023B14TU    |2010-01-06     |First, I never got to put the cam through it's ...  |0.584339779            |0.4082536174           |0.0074066037           |\n",
      "|B0023B14TU    |2010-01-07     |Easy to use and carry around. Enjoyed the pictu...  |0.9204637674           |0.0733972057           |0.0061390269           |\n",
      "|B0023B14TU    |2010-01-07     |Pros:\\                                              |0.412166425            |0.2187079              |0.369125675            |\n",
      "|B0023B14TU    |2010-01-07     |This is the second Flip we've owned.  The camer...  |0.8425501259           |0.1399941237           |0.0174557504           |\n",
      "|B0023B14TU    |2010-01-07     |I love how easy it is to learn and use this pro...  |0.9995114332           |0.0004823964           |6.170500000000000e-06  |\n",
      "|B0023B14TU    |2010-01-07     |I love this flip Video Camcorder, it's awesome....  |0.9999999947           |5.300000000000000e-09  |0                      |\n",
      "|B0023B14TU    |2010-01-07     |My son loves this - it was a hit Christmas morn...  |0.9895403556           |0.0074636816           |0.0029959628           |\n",
      "|B0023B14TU    |2010-01-07     |I am very low tech by nature, and despise readi...  |0.6897674559           |0.3101869148           |4.562930000000000e-05  |\n",
      "|B0023B14TU    |2010-01-07     |This is a great camcorder.  I have purchased tw...  |0.9888382558           |0.0044045738           |0.0067571704           |\n",
      "|B0023B14TU    |2010-01-07     |This is the best little video recorder I've eve...  |0.9999544635           |4.534940000000000e-05  |1.871000000000000e-07  |\n",
      "|B0023B14TU    |2010-01-07     |It is a great video camera, I bought it for my ...  |0.9760511406           |0.0236734897           |0.0002753697           |\n",
      "|B0023B14TU    |2010-01-08     |Read the other reviews about the flip's chargin...  |0.6140073475           |0.3693987305           |0.016593922            |\n",
      "|B0023B14TU    |2010-01-09     |I love my new Flip Ultra HD.  It's super easy t...  |0.9992581231           |0.0007415818           |2.951000000000000e-07  |\n",
      "|B0023B14TU    |2010-01-09     |This is ultra covenient - we already had a camc...  |0.8700251228           |0.1194317954           |0.0105430818           |\n",
      "|B0023B14TU    |2010-01-09     |I purchased the Flip UltraHD Camcorder after se...  |0.9950453856           |0.0042627119           |0.0006919024           |\n",
      "|B0023B14TU    |2010-01-09     |We loved our first generation flip already and ...  |0.9797875981           |0.0200794824           |0.0001329196           |\n",
      "|B0023B14TU    |2010-01-09     |Everything about this camcorder is wonderful ex...  |0.9422982925           |0.057488744            |0.0002129634           |\n",
      "|B0023B14TU    |2010-01-09     |I like the flip - it's very easy to use and you...  |0.3762299759           |0.4827081646           |0.1410618596           |\n",
      "|B0023B14TU    |2010-01-09     |I have the previous model (60 min) and loved it...  |0.9759066439           |0.006278201            |0.0178151551           |\n",
      "|B0023B14TU    |2010-01-09     |Purchased for a trip and have been using to mak...  |0.8479893121           |0.1082888237           |0.0437218642           |\n",
      "|B0023B14TU    |2010-01-10     |The camcorder has a lot of issues as other revi...  |0.8205643199           |0.0690416041           |0.110394076            |\n",
      "|B0023B14TU    |2010-01-10     |Very poor picture unless there is plenty of lig...  |0.0980080743           |0.0031147416           |0.8988771841           |\n",
      "|B0023B14TU    |2010-01-10     |Flip Ultra HD looks very, very good on my HDTV;...  |0.0036952173           |0.522681318            |0.4736234647           |\n",
      "|B0023B14TU    |2010-01-11     |There are plenty of great things to say about t...  |0.4171304529           |0.5814644629           |0.0014050842           |\n",
      "|B0023B14TU    |2010-01-12     |Would never recommend this to anyone. DO NOT BU...  |8.375700000000000e-06  |7.454520000000000e-05  |0.9999170791           |\n",
      "|B001XURP8Q    |2018-05-16     |fair                                                |0.3182017616           |0.4198232483           |0.2619749901           |\n",
      "|B001XURP8Q    |2018-05-16     |Good quality holds and play my videos.              |0.5204060764           |0.2723159656           |0.207277958            |\n",
      "|B001XURP8Q    |2018-05-16     |Not the fastest flash drive but very reliable g...  |0.6294495609           |0.2287242206           |0.1418262184           |\n",
      "|B001XURP8Q    |2018-05-16     |holds my data still after years , i don't use i...  |0.074716123            |0.3986569258           |0.5266269512           |\n",
      "|B001XURP8Q    |2018-05-16     |Good saving personal documents                      |0.6725491767           |0.1932082584           |0.1342425649           |\n",
      "|B001XURP8Q    |2018-05-17     |Not sure if it is the sliding action, but the l...  |0.0754261825           |0.611245641            |0.3133281765           |\n",
      "|B001XURP8Q    |2018-05-17     |<a data-hook=\"product-link-linked\" class=\"a-lin...  |0.4843967425           |0.3332194375           |0.18238382             |\n",
      "|B001XURP8Q    |2018-05-17     |Although I purchased a 256 GB and it would s ma...  |0.0322278634           |0.0476600191           |0.9201121175           |\n",
      "|B001XURP8Q    |2018-05-17     |I've been purchasing SanDisk sticks and cards f...  |0.3155947706           |0.1697870413           |0.514618188            |\n",
      "|B001XURP8Q    |2018-05-17     |Nice storage space for the price, but has a pre...  |0.5224219177           |0.3933990977           |0.0841789847           |\n",
      "|B001XURP8Q    |2018-05-17     |Worked good                                         |0.561254685            |0.2513808866           |0.1873644284           |\n",
      "|B001XURP8Q    |2018-05-18     |Great price and fast delivery                       |0.9199088664           |0.0728826224           |0.0072085113           |\n",
      "|B001XURP8Q    |2018-05-18     |works well. great sale price $20 each.              |0.7918341069           |0.1538746196           |0.0542912735           |\n",
      "|B001XURP8Q    |2018-05-18     |great product!!                                     |0.7466648267           |0.1649609408           |0.0883742325           |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = session.table('new_reviews_ready').select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'),\n",
    "    F.col('PROCESSED_REVIEW'),\n",
    "    F.call_udf(\n",
    "        'predict_sentiment_vect',\n",
    "        F.col('PROCESSED_REVIEW')).alias('SENTIMENT'))\n",
    "\n",
    "df = df.select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'),\n",
    "    F.col('PROCESSED_REVIEW'),\n",
    "    F.col('sentiment')['NEGATIVE'].alias('negative'),\n",
    "    F.col('sentiment')['NEUTRAL'].alias('neutral'),    \n",
    "    F.col('sentiment')['POSITIVE'].alias('positive')\n",
    ").write.save_as_table('new_reviews_scored',mode=\"overwrite\")\n",
    "\n",
    "session.table('new_reviews_scored').select(\n",
    "    F.col('product_id'),\n",
    "    F.col('review_date'),\n",
    "    F.col('product_review'),  \n",
    "    F.col('positive'),\n",
    "    F.col('neutral'),\n",
    "    F.col('negative')).show(50)"
   ]
  }
 ],
 "metadata": {
  "hex_info": {
   "author": "Simon Coombes",
   "exported_date": "Tue Oct 04 2022 15:14:44 GMT+0000 (Coordinated Universal Time)",
   "project_id": "e97d22d8-1554-4558-bd6d-4f2b72b9f9ff",
   "version": "draft"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
